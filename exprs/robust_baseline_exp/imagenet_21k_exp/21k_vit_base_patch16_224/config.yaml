
model:
    type: vit_b16_224
    kwargs:
        num_classes: 21841
        drop_path: 0.0
        dropout: 0.0
        attention_dropout: 0.0
        qkv_bias: True
        representation_size: 768
   
dist:
    sync: False

optimizer:
    type: AdamW
    no_wd:
        fc: False
        norm: False
    kwargs:
        weight_decay: 0.05

lr_scheduler:
    type: CosineEpoch
    kwargs:
        base_lr: 0.00001
        warmup_lr: 0.0005
        min_lr: 0.00001
        # warmup_steps: 2500
        # max_iter: 125000
        warmup_epoch: 2
        max_epoch: 100

label_smooth: 0.1

#mixup: 0.2
#cutmix: 1.0

ema:
    enable: True
    kwargs:
        decay: 0.9999

data:
    type: imagenet
    read_from: fake
    use_dali: false
    use_ranked: true
    batch_size: 64
    num_workers: 2
    pin_memory: False
    input_size: 224
    test_resize: 256

    train:
        root_dir: /mnt/lustrenew/liqqnew/ImageNet22k
        meta_file: /mnt/lustrenew/liqqnew/meta_train_cleaned.txt
        image_reader:
            type: pil
        sampler:
            type: ranked_iteration
        transforms:
            type: STANDARD

    test:
        root_dir: /mnt/lustrenew/liqqnew/ImageNet22k
        meta_file: /mnt/lustrenew/liqqnew/meta_val_cleaned.txt
        image_reader:
            type: pil
        sampler:
            type: distributed
        transforms:
            type: ONECROP
        evaluator:
            type: imagenet
            kwargs:
                topk: [1, 5]


saver:
    print_freq: 10
    val_freq: 5000
    save_many: False
    #pretrain:
    #    path: checkpoints/ckpt.pth.tar
    #     ignore:
    #         key:
    #             - optimizer
    #             - last_iter
    #         model:
    #             - module.fc.weight
    #             - module.fc.bias

